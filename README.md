Projet CUDA : Multiplication de Matrices Optimis√©e (Tiled Matrix Multiplication)
Ce projet impl√©mente et analyse la multiplication de matrices sur GPU en utilisant NVIDIA CUDA. L'objectif principal est de d√©montrer l'optimisation des performances gr√¢ce √† l'utilisation de la M√©moire Partag√©e (Shared Memory) et de la technique du Tiling (Tuilage).

üìÇ Structure du Projet
Le projet contient les fichiers sources suivants :

matrix_mul.cu : Impl√©mentation de base avec Tiling (Matrice 4x4, Tuiles 2x2).

comparison.cu : Benchmark comparant la vitesse de la M√©moire Globale vs M√©moire Partag√©e sur de grandes matrices (N=1024).

exo3.cu : Exercice sur des matrices 8x8 avec des Tuiles 4x4 (Occupation optimale).

exo4.cu : Exercice sur des matrices 8x8 avec des Tuiles 2x2 (Analyse de l'impact des petites tuiles).

üöÄ Pr√©requis et Compilation
Environnement
NVIDIA GPU (Test√© sur Tesla T4 via Google Colab).

CUDA Toolkit install√© (nvcc).

Compilation
Pour compiler les fichiers, utilisez le compilateur nvcc. Note : Le flag -arch=sm_75 est recommand√© pour les GPU r√©cents (comme le T4) pour √©viter les erreurs de compatibilit√© PTX.

Bash

# Compiler le code de base
nvcc -arch=sm_75 matrix_mul.cu -o matrix_mul

# Compiler le benchmark de performance
nvcc -arch=sm_75 comparison.cu -o comparison

# Compiler les exercices
nvcc -arch=sm_75 exo3.cu -o exo3
nvcc -arch=sm_75 exo4.cu -o exo4
Ex√©cution
Bash

./matrix_mul
./comparison
./exo3
./exo4
üìä Concepts Cl√©s & Analyse
1. M√©moire Globale vs M√©moire Partag√©e
L'exercice de comparaison (comparison.cu) d√©montre une diff√©rence significative de performance :

M√©moire Globale (Global Memory) : Lente (DRAM). Chaque thread va chercher ses donn√©es dans la m√©moire principale du GPU pour chaque calcul. Latence √©lev√©e (~400-800 cycles).

M√©moire Partag√©e (Shared Memory) : Tr√®s rapide (On-chip). Les threads collaborent pour charger une "tuile" de donn√©es une seule fois, puis la r√©utilisent plusieurs fois directement depuis la puce. Latence tr√®s faible (~1-2 cycles).

R√©sultat : L'impl√©mentation "Shared Memory" est nettement plus rapide car elle r√©duit drastiquement la bande passante m√©moire n√©cessaire.

2. Impact de la taille des Tuiles (Tile Size)
Les exercices 3 et 4 comparent des tuiles de tailles diff√©rentes sur une m√™me matrice :

Tuiles 4x4 (Exo 3) : Bonne balance. Le bloc contient 16 threads.

Tuiles 2x2 (Exo 4) : Moins efficace.

Raison : Un warp GPU contient 32 threads. Avec des blocs de 4 threads (2x2), le GPU sous-utilise ses capacit√©s (mauvaise "Occupancy") et perd du temps en synchronisation (__syncthreads) plus fr√©quente.

üìù Auteur
Projet r√©alis√© dans le cadre d'un laboratoire d'introduction au calcul parall√®le sur GPU (CUDA).
